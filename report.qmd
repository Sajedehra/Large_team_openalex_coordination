---
title: "OpenAlex manuscript"
author:
  - name: Sajedeh Rasti
    corresponding: true
    orcid: 0009-0007-3416-7692
    email: S.Rasti@tue.nl
    affiliations:
      - name: Eindhoven University of Technology
        department: Industrial Engineering and Innovation Sciences
        address: Den Dolech 2, 5600 MB, Eindhoven
        city: Eindhoven
  - name: Krist Vaesen
    orcid: 0000-0002-7496-7463
    email: k.vaesen@tue.nl
    affiliations:
      - name: Eindhoven University of Technology
        department: Industrial Engineering and Innovation Sciences
        address: Den Dolech 2, 5600 MB, Eindhoven
        city: Eindhoven     
  - name: Daniel Lakens
    orcid: 0000-0002-8393-5316
    email: D.Lakens@tue.nl
    affiliations:
        
        
      - name: Eindhoven University of Technology
        department: Industrial Engineering and Innovation Sciences
        address: Den Dolech 2, 5600 MB, Eindhoven
        city: Eindhoven
bibliography: references.bib  
floatsintext: true
format: pdf
editor_options: 
  chunk_output_type: console
abstract: "Abstract"
---

```{r}
#| include: false
#| label: load_data_and_function

library(dplyr)
library(ggplot2)
library(psych)
library(FSA)
library(tidyr)
library(see)
library(stringr)
library(cowplot)
library(forcats)
library(scales)
library(gridExtra)

dat<- readRDS("final_clean_dat.rds")
tem_cagr <- readRDS("growth_rate.rds")

```

## Introduction

Research questions are becoming increasingly complex [@klug_understanding_2016; @leahey_sole_2016]. To successfully answer these complex questions, researchers need to coordinate to manage interdependencies, whether temporal, spatial, financial, or epistemic [@rasti_systemic_2025]. In many fields, coordination has emerged in what is referred to as ‘big-team science’, where many authors work together on a single topic over many years. Such long term coordinated research efforts have led to impressive scientific progress, from the discovery of the Higgs boson at CERN [@aad_observation_2012], to consistent focus on patient reported outcomes in OMERACT [@tugwel_omeract_2007], and complete and accurate modelling of the climate system at WCRP [@sommeria_le_2019] [^1].

[^1]: This article, originally written in french, is translated by the authors and is avalaible at: https://www.wcrp-climate.org/documents/2019/The%20World%20Climate%20Research%20Programme%20(WCRP)%20celebrates%20its%2040th%20anniversary.pdf

A growing literature documents the rise of large-team papers and evaluates whether huge investments necessary for this type of studies are justified. Many studies find that large teams receive more citations, which can be interpreted as a proxy for greater impact [@wuchty_increasing_2007; @wu_large_2019; @uzzi_atypical_2013; @coles_prevalence_2024; @hsiehchen_multinational_2015; @bikard_exploring_2015; @porter_research_2012]. The work performed by larger teams tends to be more developmental than disruptive, builds more on prior work rather than introducing entirely novel ideas [@wu_large_2019; @coles_prevalence_2024]. However, other studies report the opposite pattern, with teams focusing more on novel ideas [@uzzi_atypical_2013].

Psychology has been adopting larger team collaboration since the success of the Reproducibility Project: Psychology [@open_science_collaboration_estimating_2015] where 270 authors collaboratively performed 100 replication studies. The field has been embraced large-team collaboration more than other disciplines in the social sciences [@coles_prevalence_2024]. As had been observed in other disciplines, big-team science projects in psychology receive more citations and as well as attention by news outlets and social media [@coles_prevalence_2024; @coles_rise_2025]. Large-team research efforts are especially common in clinical psychology, where collaborations focus on the treatment and prevention of psychological disorders [@coles_rise_2025].

However, some questions remain uninvestigated:

-   How do team-authored papers perform in terms of placement in high-visibility journals? Journal visibility has always been a key feature of academic reward culture, with some universities even offering monetary awards for publishing in outlets such as Nature and Science [Abritis et al., 2017]. It is therefore useful to examine this metric alongside citation counts.

-   Do team-authored psychology papers address topics relevant to the contemporary discussions in the field? @wu_large_2019 operationalized this relevance by immediate impact (e.g., citations in the first five years) and report higher immediate impact for large teams across domains; however, this has not been examined specifically within psychology.

-   On individual CV level, are team-authored papers the most successful outputs? At the individual level, it is unclear how team-authored papers perform relative to smaller-team papers written by the same author. This comparison could shed light on possible Matthew effects in larger teams.

-   Why do scientists opt for big-team collaborations in psychology? According to \[cite\], different types of interdependency are a primary reason scientists coordinate their work. It remains unclear to what extent these interdependencies are present in big-team psychology projects.

In this paper, we examine these issues by qualitative and quantitative analysis of papers in psychology.

## Method

Our analysis used more than 1.5 million psychology papers from the OpenAlex data snapshot [@priem_openalex_2022] from October 17, 2024. OpenAlex is an open-access scholarly metadata resource launched in 2022 as a successor to Microsoft Academic Graph. All data processing and analysis were performed in R (version 4.4.1 and higher, @r_core_team_r_2025).


OpenAlex is an open-access database of scholarly metadata and is generally more inclusive in coverage than closed-access counterparts such as Web of Science and Scopus. OpenAlex stores all scholarly documents including journal articles within the *works* entity. We iterated over the snapshot’s works section and retrieved all entries in which Psychology or Decision Science appeared as the primary or secondary field. Field assignment in OpenAlex is largely automated: a large language model (LLM) infers topics from the title, abstract, journal name, and citation network, and then maps topics to the most likely field using an algorithm developed by CWTS [@noauthor_openalex_nodate]. The OpenAlex team acknowledged that automated assignment can produce errors. Accordingly, we included Decision Science as well, as our initial checks showed that this field in OpenAlex contains many psychology-related articles, including well-known papers such as thematic analysis [@braun_using_2006] and reproducibility project [@open_science_collaboration_estimating_2015]. We then filtered out non-psychology items by title and journal-name keywords (e.g., “symposium”, “table of content”, “Synthesiology”, and “Aerospace”). Later during the qualitative analysis, we discovered more keywords and added them to the list and rerun all the analyses. @fig-preprocesing shows the preprocessing and cleaning steps.


:::
![](flowchart.png){#fig-preprocesing}
Data Cleaning Process
:::

### Metrics of Interest

To measure scholarly impact, we use (a) total citation counts, (b) second-year citation counts, and (c) publication-date–corrected citation counts. We chose not to adjust for self-citations for two reasons. First, we lacked reliable self-citation data for all papers in the dataset. Second, based on our own prior observations, large-team papers often bring together many of the field’s active scholars (e.g., the case of ManyPrimates) and produce results that serve as reference points for an entire research line. Discounting self-citations would therefore risk understating the field-level impact that is central to this study. In this document, analyses use second-year citation counts; robustness checks with alternative metrics are reported in the Supplementary Materials.

We operationalized the impact of journals using the SCImago Journal Rank (SJR). We chose SJR because it is open access (whereas Impact Factor data are typically proprietary) and offers broader coverage (31,125 journals vs. 22,249 in Web of Science). Both metrics divide journal's citations by its output over a specific time window. But unlike the Impact Factor, SJR weighs citations by the prestige of the citing journal which is estimated via a PageRank algorithm on the journal citation network, and discounts self-citations. It also uses all documents in the denominator, whereas Impact Factor counts only “citable items”, such as articles and reviews [@oosthuizen_alternatives_2014; @falagas_comparison_2008]. For comparison, in 2024, *Nature* has an Impact Factor of 48.5 and an SJR of 18.288. Despite greater coverage, only XX records in our dataset had matching journals in the SJR records. Therefore, all results based on journal prestige is based on this subset. [add info on randomness of the missing values] 

### Team size

In this paper, we did not impose a single threshold for “big-team” papers. Wherever possible, we analyzed the full distribution of the number of co-authors. We grouped papers into single-, two-, three-, four-, and five-author papers, and for the remaining papers we exploratively analyzed alternative bin widths of 5, 10, and 15 authors. In each team-size bin, the final bin captured very large teams (＞100 authors); except for the 10-author scheme, where the last bin began at ≥96 authors to preserve 10-author intervals. In the remainder of this document, we report results using a bin width of 5 authors; robustness checks with other grouping schemes are provided in the Supplementary Materials and reveal that bin-size does not matter for the main conclusions.

### Qualitative checks

We randomly selected 1000 papers with more than 20 authors for qualitative checks. Our initial plan was to code 500 records, but since some papers did not meet the inclusion criteria, we extended the screening and ended up analyzing 640 papers in total. All abstracts were single-coded by the first author. Ambiguous cases (n = 8) were reviewed and discussed with the last author. Coding was performed in MAXQDA 2022 software (version 22.1.0, VERBI GmbH, Berlin, Germany [it should be cited]).


## Results

Results are organized over the following themes:

**Big-team science is an increasing phenomenon**

@fig-author-trend shows an increased frequency of the big-team papers in Psychology. Over the last decade, the papers with a larger author list increased dramatically, and in some cases had more than double the growth rates of smaller author groups.


```{r}
#| label: fig-author-trend
#| fig-cap: "Growth of big team papers"
#| warning: false
#| message: false
#| echo: false
#| fig-align: center


heat<- dat|>  mutate(pubyear = as.integer(format(as.Date(pubdate), "%Y")))|>
  ggplot( aes(x = pubyear, y = n_author)) +
  geom_bin2d(binwidth = c(1, 1)) +
  scale_fill_viridis_c(trans = "log10") +  
  labs(x = "Year", y = "Team size", fill = "Count") +
  theme_classic()


plot_growth <- tem_cagr |>
  filter(!is.na(author_group_five))|>
  ggplot(aes(x = growth_rate, y = author_group_five)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_segment(aes(x = 0, xend = growth_rate, y = author_group_five, yend = author_group_five), color="grey") +
  geom_point(size = 3, color="orange") +
  scale_x_continuous(labels = label_number(suffix = "%")) + 
  labs(x = "Ten-year growth rate (%)", y = "Number of authors") +
  theme_classic()+
  theme(axis.line.x = element_line(size = 0.5, linetype = "solid", colour = "grey"))

plot_grid(heat,plot_growth,nrow = 2, ncol = 1, labels = c("A", "B"))
```

**Larger teams receive more citations**

```{r}
#| warning: false
#| message: false
#| echo: false

n_used <- sum(complete.cases(dat$n_author, dat$citation))

cit_corr<-cor.test(dat$n_author, dat$citation, method = "spearman", use = "complete.obs")
cit2_corr<-cor.test(dat$n_author, dat$citation_2y, method = "spearman", use = "complete.obs")
cityear_corr<-cor.test(dat$n_author, dat$corr_citation, method = "spearman", use = "complete.obs")

drop0 <- function(x) sub("^0\\.", ".", x)
apa_p <- function(p) ifelse(p < .001, "p < .001",
                            paste0("p = ", drop0(sprintf("%.3f", p))))
rho1 <- drop0(sprintf("%.3f", unname(cit_corr$estimate)))
p_txt1 <- apa_p(cit_corr$p.value)

rho2 <- drop0(sprintf("%.3f", unname(cit2_corr$estimate)))
p_txt2 <- apa_p(cit2_corr$p.value)

rho3 <- drop0(sprintf("%.3f", unname(cityear_corr$estimate)))
p_txt3 <- apa_p(cityear_corr$p.value)
```

Overall, bigger teams receive more citations ($\rho$ = `r rho1`, `r p_txt1` (N = `r n_used`)). This relationship is stronger for the second-year citation count ($\rho$ = `r rho2`, `r p_txt2` (N = `r n_used`)) and corrected citations based on publication date ($\rho$ = `r rho3`, `r p_txt3` (N = `r n_used`)). Under the grouping scheme, the median and mean citation counts increased fairly consistently up to the 45-author group, followed by many fluctuations (see @fig-des-citation).


```{r}
#| label: fig-des-citation
#| fig-cap: "Team size and citation counts"
#| warning: false
#| message: false
#| echo: false
#| fig-align: center

box<- dat |>
  filter(!is.na(author_group_five))|>
  group_by(author_group_five) |>
  ggplot(aes(x = author_group_five, y = citation_2y)) +
  geom_boxplot(outlier.shape = NA, fill = "lightblue") +
  stat_summary(
    fun = mean, geom = "point", color = "red", size = 2
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  coord_cartesian(ylim = c(0, 250))+
  labs(x = "Author groups", y= "Second-year citation count")


percentile<- dat|> filter(!is.na(author_group_five))|>group_by(author_group_five)|> summarise(value= quantile(citation_2y, c(.7, .8, .90, .99)))|> mutate(percentile = as.factor(c(.7, .8,.9,.99)))|>
  ggplot(aes(x=author_group_five, y = value, color = percentile))+
  geom_point(size= 2)+
  theme_classic()+
  labs(x= "Author group", y= "Second-year citation")+
  coord_cartesian(ylim = c(0, 1000))

#plot_grid(box, percentile,nrow = 2, ncol = 1, labels = c("A", "B"))
grid.arrange(box, percentile)
```

**Larger teams are published in more prestigious journals**

Even though rank correlation among number of authors and SJR value is not high ($\rho$ = `r rho1`), @Fig shows that mean and median SJR value increase consistently until 56-60 authors bin, then fluctuate for larger groups.

**Papers authored by larger teams are less risky in terms of prestigue and impact**

Across the 70th, 80th, and 90th percentiles, papers from smaller teams consistently receive fewer citations than of larger teams, showcasing the higher risks of low gain in smaller teams. This trend only changes for 99th and 99.99th percentile showcasing that receiving more citation and impact only happens in few cases (see @fig)

**Big-team papers are consistently more successful compare to other publications of the same author**

Pairwise comparisons of citation counts show that papers with more than 20 authors have higher mean citation ranks than smaller teams. Therefore, we use 20 authors as the cutoff for “big teams” (>=20) in this comparison. We also find that very small teams (1–5 authors) consistently differ from moderate teams (6–19 authors), and, conceptually, overlap among very small teams is less likely. As a result, we compare big teams (>=20) with very small teams (1–5). Robustness checks for other small team definitions are provided in the Supplementary Materials.

the results shows 

**Qualitative checks**

## Discussion

\newpage

## References
